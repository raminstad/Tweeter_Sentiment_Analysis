{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8683e060",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61f9494f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import contractions\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split,cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863cc6aa",
   "metadata": {},
   "source": [
    "## Load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "365c2ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63391944",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea13b529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is used for data cleaning, it has 4 methods and they are used for\n",
    "# Contractions, stop words, lemmatizer, porterStemmer\n",
    "class grammer(object):\n",
    "    def __init__(self,txt):\n",
    "        self.txt=txt\n",
    "    \n",
    "    def contraction(self):\n",
    "        t = self.txt\n",
    "        expanded_words = []    \n",
    "        for word in t.split():\n",
    "            expanded_words.append(contractions.fix(word))   \n",
    "        expanded_text = ' '.join(expanded_words)    \n",
    "\n",
    "        return expanded_text\n",
    "\n",
    "    def stop_word(self):\n",
    "        example_sent = self.txt\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        word_tokens = word_tokenize(example_sent)\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        filtered_sentence = []\n",
    "        for w in word_tokens:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        expanded_text = ' '.join(filtered_sentence)    \n",
    "        \n",
    "        return expanded_text\n",
    "\n",
    "    def lemmatize(self):\n",
    "        wnl = WordNetLemmatizer()\n",
    "        string = self.txt\n",
    "        list2 = nltk.word_tokenize(string)\n",
    "        lemmatized_string = ' '.join([wnl.lemmatize(words) for words in list2])\n",
    "\n",
    "        return lemmatized_string  \n",
    "    def stem(self):\n",
    "        ps = PorterStemmer()\n",
    "        sentence = self.txt\n",
    "        words = word_tokenize(sentence)\n",
    "        g=[]  \n",
    "        for w in words:\n",
    "            g.append(ps.stem(w))\n",
    "        final=' '.join(s for s in g)\n",
    "        \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f35814a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is used as a pipeline and is inhereting from BaseEstimator and TransformerMixin\n",
    "# The reason that we need to build this class as a pipeline is that we can also clean our test data with this pipeline\n",
    "class DataFrameImputer(BaseEstimator,TransformerMixin):\n",
    "\n",
    "    def __init__(self,variable1):\n",
    "        self.variable1=variable1\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df= X.copy()\n",
    "        # removing hashtags\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: x.replace('#',''))\n",
    "        # removing any @ or links\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \",str(x)).split()))\n",
    "        # removing digits\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: re.sub(r'[^a-zA-Z0-9]', ' ', x))\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))\n",
    "        # lower case everything\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: \" \".join(x.split()))\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: x.lower())\n",
    "        # apply the grammer class that we defined above\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: grammer(x).contraction())\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: grammer(x).stop_word())\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: grammer(x).lemmatize())\n",
    "        df[self.variable1]=df[self.variable1].apply(lambda x: grammer(x).stem())\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e104e5b7",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36a85aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the pipeline from the class that we defined above\n",
    "raw_data=Pipeline(steps=[('df',DataFrameImputer('tweet'))])\n",
    "# fit and transform our data with the pipeline\n",
    "df=raw_data.fit_transform(df)\n",
    "# converting the dataframe into a dictionary\n",
    "final=df.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae8c64fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our data set had two columns and they were the text and the number\n",
    "# 0 was positive and 1 was negative\n",
    "# we make a class and we name the text txt and number num and we also defined sent which is either positive or negative\n",
    "class convert(object):\n",
    "    def __init__(self,txt,num):\n",
    "        self.txt=txt\n",
    "        self.num=num\n",
    "        self.sent=self.sentiment()\n",
    "    def sentiment(self):\n",
    "        if self.num==0:\n",
    "            return 'POSITIVE'\n",
    "        else:\n",
    "            return 'NEGATIVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e267914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this function we apply the convert class that we defined above and we also make sure \n",
    "# to have even amount of positve and negative data, so our model won't be bias\n",
    "# and also the f1_score will be close for both positive and negative\n",
    "def even(dic):\n",
    "    data=[]\n",
    "    negative=[]\n",
    "    positive=[]\n",
    "    for i in dic:\n",
    "        data.append(convert(i['tweet'],i['label']))\n",
    "    for i in range(len(data)):\n",
    "        if data[i].sent=='NEGATIVE':\n",
    "            negative.append(data[i])\n",
    "        else:\n",
    "            positive.append(data[i])\n",
    "\n",
    "    positive=positive[:len(negative)]\n",
    "    full=negative+positive\n",
    "    return full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3946811b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying the function\n",
    "data=even(final)\n",
    "\n",
    "# shuffling the data\n",
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ab6534",
   "metadata": {},
   "source": [
    "## train, test, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5be5b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split([x.txt for x in data],[x.sent for x in data]\n",
    "                                               ,test_size=.20\n",
    "                                               ,random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0629352",
   "metadata": {},
   "source": [
    "## model building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d56818",
   "metadata": {},
   "source": [
    "we will use TfidfVectorizer() as our text vectorizer and RandomForestClassifier() as our classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33b5be9",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc13f172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.86887115, 0.87459106])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# making a pipeline for our models\n",
    "pipe_rf=Pipeline(steps=[('vec',TfidfVectorizer()),\n",
    "                     ('rf',RandomForestClassifier())])\n",
    "\n",
    "# fit\n",
    "pipe_rf.fit(X_train,y_train)\n",
    "# predict\n",
    "y_pred_rf=pipe_rf.predict(X_test)\n",
    "# f1_score\n",
    "f1_score(y_test,y_pred_rf,average=None,labels=['POSITIVE','NEGATIVE'])\n",
    "# we can see that our model is not bias towards any of our variables \n",
    "# and it can predict positive and negative almost the same"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb667c",
   "metadata": {},
   "source": [
    "#### Cross validation for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5bc749b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8305122858343317"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RF=cross_val_score(pipe_rf,X_train,y_train,cv=10,scoring='accuracy').mean()\n",
    "RF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9adefc4",
   "metadata": {},
   "source": [
    "#### Parameter tuning for Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5174355",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_rf = {\n",
    "\"rf__bootstrap\": [True,False],\n",
    "\"rf__max_depth\": list(range(80,220,20)), \n",
    "\"rf__max_features\": [\"auto\",\"sqrt\"],\n",
    "\"rf__min_samples_split\": [2,5,10],\n",
    "\"rf__min_samples_leaf\": [1,2,4,6,8,10],\n",
    "\"rf__n_estimators\":  [int(x) for x in np.linspace(start = 200, stop = 2000, num =10)]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b8a4b05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8338556822956381"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand_rf=RandomizedSearchCV(pipe_rf,param_rf,cv=10,n_iter=10,scoring='accuracy')\n",
    "rand_rf.fit(X_train,y_train)\n",
    "rand_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aebf5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=rand_rf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c257b9e9",
   "metadata": {},
   "source": [
    "#### cross validation of the tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2d6419f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8338564603725432"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_cross=cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy').mean()\n",
    "final_cross"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6709978",
   "metadata": {},
   "source": [
    "#### evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "19b021e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8706800445930881"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cd7b71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SENTIMENT.pkl','wb') as f:\n",
    "    pickle.dump(model,f)\n",
    "\n",
    "# load\n",
    "with open('SENTIMENT.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2537d8",
   "metadata": {},
   "source": [
    "### Predicting the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7379863a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')\n",
    "test1=test.copy()\n",
    "\n",
    "# We use the raw_data pipeline that we created earlier to fit_transform the test data \n",
    "test=raw_data.fit_transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63a44ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting each tweet \n",
    "test['predictions']=test['tweet'].apply(lambda x: model.predict([x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d80822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop the the modified tweet where we used raw_data pipeline on\n",
    "test.drop(['tweet'],1,inplace=True)\n",
    "\n",
    "# We merge the test and test1 so we can have the original tweet text that are not modified\n",
    "final_df=pd.merge(test,test1,on=['id'],how='inner')\n",
    "\n",
    "# Converting ndarray to strings\n",
    "final_df['predictions']=final_df['predictions'].apply(lambda x: ','.join(str(a) for a in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6c31fb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>predictions</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id predictions                                              tweet\n",
       "0  31963    POSITIVE  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964    NEGATIVE   @user #white #supremacists want everyone to s...\n",
       "2  31965    POSITIVE  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966    POSITIVE  is the hp and the cursed child book up for res...\n",
       "4  31967    POSITIVE    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
